# coalitions-for-tool-use - Assessing Critiquing

This research was performed by IBM Research UK and investigates if coalitions of open-sourced, pretrained (non-fine-tuned) Large Language Models, can work together to assist in complex workflows through agentic augmentation with external tools. 

In this directory we share our assessment strategy for examining which pre-trained model performs the best at critiquing plans for use within our coalition system.

For assessing a systems ability to operate in the tool use domain it has to excel in the following sub-tasks:
1. (Planning) Given an intent/prompt, plan the tools to use (LLM driven)
1. (Slot filling/Execution) Slot fill the parameters for the tools to be used (LLM driven)
1. (Slot filling/Execution) Execute the tool with the inferred parameters (System/programmatically driven)
1. (Response Forming) Formulate a meaningful response to the initial query, based on the responses of the executed tools (LLM driven)

The ability to decompose and assign these sub tasks to different models, permits a more seperable approach and enables use of additional modules. As such our coalition utilises an additional module named JSON RAG which is used to filter large JSON responses from API executions. It is utilised both in the Executor and the Response Former.

In this directory we assess the model best suited to the JSON RAG task for filtering executed tool responses. 

## Experimental Setup
Dataset Used:
- Custom Data - [link](./testcases_orig.json).

The models assessed:
- `mistralai/mistral-7b-instruct-v0-2`
- `mistralai/mixtral-8x7b-instruct-v01`
- `meta-llama/llama-2-70b-chat`
- `google/flan-ul2`
- `codellama/codellama-34b-instruct`

##Â Strategy:

For this assessment a dataset has been built containing the JSON RAG prompts and the expected fields to be selected for a given task. 

The task of the model is to select the relevant fields from the available list based on the input prompt.

## Running the Evaluations

The summarised results can be regenerated by using the `run_json_rag_prompts.py` script to recreate the evaluation and the metrics collected. 

### Reproducing the results:

1. Follow the guide in the root directory to setup your local environment
1. Change into this directory (if in the root directory): `cd assessing-jsonrag`
1. Based on your access, you may need to setup connection to a watsonx.ai deployment and update the provided `run_json_rag_prompts.py` script to utilise the `ibm-watsonx-ai` SDK - see [here](https://ibm.github.io/watsonx-ai-python-sdk/install.html).
1. Export connection to your watsonx.ai instance. For example:
    ```bash
    export GENAI_KEY=<apikey to connect to watsonx.ai instance>
    export GENAI_API=<address of watsonx.ai instance>
    ```
1. Choose the model to test, for example:
    ```python
    # MODEL_ID="mistralai/mistral-7b-instruct-v0-2"
    # MODEL_ID="mistralai/mixtral-8x7b-instruct-v01"
    # MODEL_ID="meta-llama/llama-2-70b-chat"
    # MODEL_ID="google/flan-ul2"
    # MODEL_ID="codellama/codellama-34b-instruct"
    ```
1. Run the evaluation script, for example:
    ```bash
    MODEL_ID="google/flan-ul2" python run_json_rag_prompts.py
    ```
1. Examine the generated presented in the bash `stdout` or terminal. 

For any queries or feedback please email: prattyush.mangal@ibm.com
